{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7965fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 ‚Äî Configuration and Imports (FREE INSTRUCTION MODELS)\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,  # Changed from Seq2Seq to CausalLM\n",
    "    Trainer,  # Changed from Seq2SeqTrainer\n",
    "    TrainingArguments,  # Changed from Seq2SeqTrainingArguments\n",
    "    DataCollatorForLanguageModeling  # Changed collator\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "\n",
    "# üöÄ BEST FREE INSTRUCTION MODELS FOR HUMANIZATION\n",
    "FREE_MODELS = [\n",
    "    {\n",
    "        \"name\": \"microsoft/DialoGPT-large\",\n",
    "        \"description\": \"Excellent for conversational humanization\",\n",
    "        \"size\": \"774M\",\n",
    "        \"strengths\": \"Natural dialogue, fast training\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mistralai/Mistral-7B-v0.1\", \n",
    "        \"description\": \"High-quality instruction following\",\n",
    "        \"size\": \"7B\",\n",
    "        \"strengths\": \"Excellent reasoning, no gating\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"description\": \"Fast and efficient for limited resources\",\n",
    "        \"size\": \"1.1B\", \n",
    "        \"strengths\": \"Very fast, good quality\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"microsoft/GRIN-MoE\",\n",
    "        \"description\": \"Mixture of experts for diverse styles\",\n",
    "        \"size\": \"2.7B\",\n",
    "        \"strengths\": \"Multiple writing styles\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# AUTO-SELECT BEST MODEL BASED ON AVAILABLE MEMORY\n",
    "def select_best_model():\n",
    "    \"\"\"Automatically select the best free model based on system capabilities\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"üñ•Ô∏è  GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        if gpu_memory >= 20:\n",
    "            # High memory - use best quality\n",
    "            return \"mistralai/Mistral-7B-v0.1\"\n",
    "        elif gpu_memory >= 8:\n",
    "            # Medium memory - balanced option\n",
    "            return \"microsoft/DialoGPT-large\"\n",
    "        else:\n",
    "            # Limited memory - efficient option\n",
    "            return \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    else:\n",
    "        print(\"üíª Using CPU - selecting efficient model\")\n",
    "        return \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "MODEL_NAME = select_best_model()\n",
    "print(f\"\\nü§ñ AUTO-SELECTED MODEL: {MODEL_NAME}\")\n",
    "\n",
    "# Find model info\n",
    "model_info = next((m for m in FREE_MODELS if m[\"name\"] == MODEL_NAME), None)\n",
    "if model_info:\n",
    "    print(f\"üìä Size: {model_info['size']}\")\n",
    "    print(f\"üí™ Strengths: {model_info['strengths']}\")\n",
    "    print(f\"üìù Description: {model_info['description']}\")\n",
    "\n",
    "# Alternative models if auto-selection fails\n",
    "FALLBACK_MODELS = [\n",
    "    \"microsoft/DialoGPT-medium\",  # 355M - Always works\n",
    "    \"gpt2-large\",                 # 774M - Reliable fallback\n",
    "    \"distilgpt2\"                  # 82M - Minimal option\n",
    "]\n",
    "\n",
    "# OPTIMAL DATASETS FOR INSTRUCTION-FOLLOWING HUMANIZATION  \n",
    "DATASET_OPTIONS = [\n",
    "    \"HuggingFaceH4/ultrachat_200k\",  # High-quality conversational data\n",
    "    \"microsoft/orca-math-word-problems-200k\",  # Natural problem descriptions\n",
    "    \"argilla/distilabel-math-preference-dpo\"  # Human preference data\n",
    "]\n",
    "\n",
    "# OPTIMIZED SETTINGS FOR FREE MODELS\n",
    "MAX_LENGTH = 1024  # Reduced for efficiency\n",
    "BATCH_SIZE = 2  # Conservative for memory\n",
    "GRADIENT_ACCUMULATION = 4  # Effective batch size = 8\n",
    "EPOCHS = 3  # More epochs for smaller models\n",
    "LR = 5e-5  # Higher LR for faster convergence\n",
    "SEED = 42\n",
    "MAX_SAMPLES = 8000  # Focused training set\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# OPTIMIZED LORA CONFIG FOR FREE MODELS\n",
    "LORA_R = 16  # Balanced rank\n",
    "LORA_ALPHA = 32  # 2x rank\n",
    "LORA_DROPOUT = 0.05  # Low dropout\n",
    "\n",
    "# Set seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"\\nüéâ\" * 30)\n",
    "print(\"FREE INSTRUCTION MODEL HUMANIZATION SETUP\")\n",
    "print(\"üéâ\" * 30)\n",
    "\n",
    "print(f\"\\n‚úÖ SELECTED FREE MODEL: {MODEL_NAME}\")\n",
    "print(\"üöÄ No authentication required!\")\n",
    "print(\"‚ö° Optimized for humanization tasks\")\n",
    "print(\"üíæ Memory-efficient configuration\")\n",
    "\n",
    "print(f\"\\nüîß DEVICE: {DEVICE}\")\n",
    "print(f\"üß† PyTorch: {torch.__version__}\")\n",
    "print(f\"‚ö° CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéØ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"üñ•Ô∏è  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "\n",
    "print(f\"\\nüìà OPTIMIZED CONFIG:\")\n",
    "print(f\"  Max length: {MAX_LENGTH}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LR}\")\n",
    "print(f\"  LoRA rank: {LORA_R}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES:,}\")\n",
    "\n",
    "print(f\"\\nüí° WHY FREE MODELS WORK GREAT:\")\n",
    "print(\"  ‚úÖ No authentication barriers\")\n",
    "print(\"  ‚úÖ Optimized configurations provided\")\n",
    "print(\"  ‚úÖ Fast training and inference\")\n",
    "print(\"  ‚úÖ Excellent humanization results\")\n",
    "print(\"  ‚úÖ Multiple fallback options\")\n",
    "\n",
    "print(f\"\\nüìã AVAILABLE FREE MODELS:\")\n",
    "for model in FREE_MODELS:\n",
    "    status = \"üü¢ SELECTED\" if model[\"name\"] == MODEL_NAME else \"‚ö™\"\n",
    "    print(f\"  {status} {model['name']} ({model['size']})\")\n",
    "    print(f\"      {model['description']}\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to load {MODEL_NAME}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca97f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 ‚Äî Load Instruction-Following Dataset for Llama\n",
    "print(\"üîÑ Loading high-quality conversational dataset for Llama-3.1...\")\n",
    "\n",
    "# STRATEGY: Use instruction-following datasets that Llama understands natively\n",
    "datasets_to_try = [\n",
    "    {\n",
    "        \"name\": \"HuggingFaceH4/ultrachat_200k\",\n",
    "        \"description\": \"High-quality multi-turn conversations - perfect for humanization\",\n",
    "        \"field\": \"messages\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"microsoft/orca-math-word-problems-200k\", \n",
    "        \"description\": \"Natural problem descriptions with human explanations\",\n",
    "        \"field\": \"question\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"argilla/distilabel-math-preference-dpo\",\n",
    "        \"description\": \"Human preference data for natural responses\",\n",
    "        \"field\": \"instruction\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tatsu-lab/alpaca_cleaned\",\n",
    "        \"description\": \"Clean instruction-following dataset\",\n",
    "        \"field\": \"instruction\"\n",
    "    }\n",
    "]\n",
    "\n",
    "ds = None\n",
    "dataset_info = None\n",
    "\n",
    "for dataset_config in datasets_to_try:\n",
    "    try:\n",
    "        print(f\"\\nüîÑ Trying {dataset_config['name']}...\")\n",
    "        print(f\"   Purpose: {dataset_config['description']}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        ds_all = load_dataset(dataset_config['name'])\n",
    "        \n",
    "        # Get training split\n",
    "        if \"train\" in ds_all:\n",
    "            ds = ds_all[\"train\"]\n",
    "        elif \"train_sft\" in ds_all:\n",
    "            ds = ds_all[\"train_sft\"]  # Some datasets use this\n",
    "        else:\n",
    "            # Use first available split\n",
    "            split_name = list(ds_all.keys())[0]\n",
    "            ds = ds_all[split_name]\n",
    "            print(f\"   Using split: {split_name}\")\n",
    "        \n",
    "        dataset_info = dataset_config\n",
    "        print(f\"‚úÖ Successfully loaded {dataset_config['name']}\")\n",
    "        print(f\"üì¶ Dataset size: {len(ds):,} samples\")\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not load {dataset_config['name']}: {str(e)[:100]}...\")\n",
    "        continue\n",
    "\n",
    "# Fallback to reliable paraphrase dataset\n",
    "if ds is None:\n",
    "    print(\"\\nüîÑ Falling back to ChatGPT paraphrases (converted for instruction format)...\")\n",
    "    try:\n",
    "        ds_all = load_dataset(\"humarin/chatgpt-paraphrases\")\n",
    "        ds = ds_all[\"train\"]\n",
    "        dataset_info = {\n",
    "            \"name\": \"humarin/chatgpt-paraphrases\",\n",
    "            \"description\": \"Paraphrase pairs converted to instruction format\",\n",
    "            \"field\": \"text\"\n",
    "        }\n",
    "        print(f\"‚úÖ Successfully loaded fallback dataset\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"‚ùå Could not load any dataset! Last error: {e}\")\n",
    "\n",
    "print(f\"\\nüìä DATASET LOADED: {dataset_info['name']}\")\n",
    "print(f\"üìù Description: {dataset_info['description']}\")\n",
    "print(f\"üì¶ Dataset size: {len(ds):,} samples\")\n",
    "print(f\"üè∑Ô∏è  Available columns: {ds.column_names}\")\n",
    "\n",
    "# Show sample record\n",
    "sample = ds[0]\n",
    "print(f\"\\nüîç SAMPLE RECORD STRUCTURE:\")\n",
    "for key in sample.keys():\n",
    "    value = sample[key]\n",
    "    if isinstance(value, (list, dict)):\n",
    "        print(f\"  {key}: {type(value).__name__} with {len(value) if hasattr(value, '__len__') else '?'} items\")\n",
    "        if isinstance(value, list) and len(value) > 0:\n",
    "            print(f\"    Sample item: {str(value[0])[:100]}...\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"    Keys: {list(value.keys())}\")\n",
    "    else:\n",
    "        value_preview = str(value)[:150]\n",
    "        if len(str(value)) > 150:\n",
    "            value_preview += \"...\"\n",
    "        print(f\"  {key}: {value_preview}\")\n",
    "\n",
    "print(f\"\\nüéØ Primary field: '{dataset_info['field']}'\")\n",
    "print(f\"‚úÖ Dataset ready for Llama-3.1 instruction processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66adefb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'sau (Python 3.11.14)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n sau ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Cell 2.5 ‚Äî Inspect Dataset Structure (UPDATED FOR ACTUAL STRUCTURE)\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample = ds[0]\n",
    "print(f\"\\nColumns: {ds.column_names}\")\n",
    "print(f\"\\nSample record keys: {list(sample.keys())}\")\n",
    "\n",
    "# Inspect actual fields that exist\n",
    "for key, value in sample.items():\n",
    "    print(f\"\\n--- '{key}' field ---\")\n",
    "    print(f\"Type: {type(value)}\")\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        preview = value[:200] if len(value) > 200 else value\n",
    "        print(f\"Value: {preview}...\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"List length: {len(value)}\")\n",
    "        if len(value) > 0:\n",
    "            print(f\"First item type: {type(value[0])}\")\n",
    "            if isinstance(value[0], dict):\n",
    "                print(f\"First item keys: {list(value[0].keys())}\")\n",
    "                print(f\"First item: {value[0]}\")\n",
    "            else:\n",
    "                preview = str(value[0])[:150] if len(str(value[0])) > 150 else str(value[0])\n",
    "                print(f\"First item: {preview}\")\n",
    "            \n",
    "            if len(value) > 1:\n",
    "                if isinstance(value[1], dict):\n",
    "                    print(f\"Second item: {value[1]}\")\n",
    "                else:\n",
    "                    preview = str(value[1])[:150] if len(str(value[1])) > 150 else str(value[1])\n",
    "                    print(f\"Second item: {preview}\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"Dict keys: {list(value.keys())}\")\n",
    "        for k, v in value.items():\n",
    "            preview = str(v)[:100] if len(str(v)) > 100 else str(v)\n",
    "            print(f\"  {k}: {preview}\")\n",
    "    else:\n",
    "        print(f\"Value: {value}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS FOR LLAMA PROCESSING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Determine the best field to use for conversation extraction\n",
    "if 'messages' in sample:\n",
    "    print(\"‚úÖ Found 'messages' field - Perfect for conversation data!\")\n",
    "    messages = sample['messages']\n",
    "    if isinstance(messages, list) and len(messages) > 0:\n",
    "        print(f\"üìù Messages count: {len(messages)}\")\n",
    "        for i, msg in enumerate(messages[:3]):  # Show first 3 messages\n",
    "            if isinstance(msg, dict):\n",
    "                role = msg.get('role', 'unknown')\n",
    "                content = msg.get('content', str(msg))\n",
    "                content_preview = content[:100] if len(content) > 100 else content\n",
    "                print(f\"  Message {i+1}: {role} - {content_preview}...\")\n",
    "\n",
    "if 'prompt' in sample:\n",
    "    prompt_preview = sample['prompt'][:200] if len(sample['prompt']) > 200 else sample['prompt']\n",
    "    print(f\"\\n‚úÖ Found 'prompt' field: {prompt_preview}...\")\n",
    "\n",
    "print(f\"\\nüéØ Dataset type identified: {dataset_info['name']}\")\n",
    "print(\"‚úÖ Ready for Llama conversation processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 ‚Äî Llama-3.1 Instruction Dataset Processing (UPDATED FOR ACTUAL STRUCTURE)\n",
    "import ast\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Llama-3.1 instruction format\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert at rewriting formal, robotic text to sound natural and human. Make the text conversational, warm, and engaging while preserving the original meaning.\"\"\"\n",
    "\n",
    "def format_llama_instruction(formal_text, natural_text):\n",
    "    \"\"\"Format training data in Llama-3.1 instruction format\"\"\"\n",
    "    instruction = f\"Rewrite this formal text to sound more natural and human:\\n\\n{formal_text}\"\n",
    "    \n",
    "    conversation = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM_PROMPT}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{natural_text}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "def create_formal_version(text):\n",
    "    \"\"\"Convert natural text to formal/robotic version (enhanced)\"\"\"\n",
    "    \n",
    "    # Expand contractions\n",
    "    contractions = {\n",
    "        \"don't\": \"do not\", \"won't\": \"will not\", \"can't\": \"cannot\",\n",
    "        \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \n",
    "        \"couldn't\": \"could not\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "        \"wasn't\": \"was not\", \"weren't\": \"were not\", \"haven't\": \"have not\",\n",
    "        \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"I'm\": \"I am\",\n",
    "        \"you're\": \"you are\", \"we're\": \"we are\", \"they're\": \"they are\",\n",
    "        \"it's\": \"it is\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
    "        \"let's\": \"let us\", \"here's\": \"here is\", \"what's\": \"what is\"\n",
    "    }\n",
    "    \n",
    "    formal_text = text\n",
    "    for contraction, expansion in contractions.items():\n",
    "        formal_text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expansion, formal_text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Replace casual words with formal equivalents\n",
    "    replacements = {\n",
    "        r'\\bget\\b': 'obtain', r'\\bgot\\b': 'obtained', r'\\bgetting\\b': 'obtaining',\n",
    "        r'\\bmake\\b': 'create', r'\\bmade\\b': 'created', r'\\bmaking\\b': 'creating',\n",
    "        r'\\bshow\\b': 'demonstrate', r'\\bshowed\\b': 'demonstrated',\n",
    "        r'\\bthink\\b': 'believe', r'\\bthought\\b': 'believed',\n",
    "        r'\\bwant\\b': 'desire', r'\\bwanted\\b': 'desired',\n",
    "        r'\\bneed\\b': 'require', r'\\bneeded\\b': 'required',\n",
    "        r'\\bstart\\b': 'commence', r'\\bstarted\\b': 'commenced',\n",
    "        r'\\bhelp\\b': 'assist', r'\\bhelped\\b': 'assisted',\n",
    "        r'\\bbig\\b': 'significant', r'\\bhuge\\b': 'substantial',\n",
    "        r'\\btiny\\b': 'minimal', r'\\bsmall\\b': 'limited',\n",
    "        r'\\blots of\\b': 'numerous', r'\\ba lot of\\b': 'numerous',\n",
    "        r'\\bgreat\\b': 'excellent', r'\\bgood\\b': 'satisfactory',\n",
    "        r'\\bbad\\b': 'unsatisfactory', r'\\bawesome\\b': 'exceptional'\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in replacements.items():\n",
    "        formal_text = re.sub(pattern, replacement, formal_text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Add more formal sentence structure\n",
    "    formal_text = re.sub(r'\\bSo,\\b', 'Therefore,', formal_text)\n",
    "    formal_text = re.sub(r'\\bBut\\b', 'However,', formal_text)\n",
    "    formal_text = re.sub(r'\\bAlso,\\b', 'Additionally,', formal_text)\n",
    "    \n",
    "    return formal_text\n",
    "\n",
    "def extract_text_from_record(record, dataset_name):\n",
    "    \"\"\"Extract usable text from different dataset formats - UPDATED FOR ACTUAL STRUCTURE\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    # Handle the actual dataset structure we found\n",
    "    if \"messages\" in record:\n",
    "        messages = record[\"messages\"]\n",
    "        if isinstance(messages, list):\n",
    "            for msg in messages:\n",
    "                if isinstance(msg, dict):\n",
    "                    role = msg.get(\"role\", \"\")\n",
    "                    content = msg.get(\"content\", \"\")\n",
    "                    \n",
    "                    # Extract natural human responses (assistant or user messages)\n",
    "                    if role in [\"assistant\", \"user\"] and content:\n",
    "                        content = str(content).strip()\n",
    "                        if len(content.split()) > 8:  # Reasonable length\n",
    "                            texts.append(content)\n",
    "    \n",
    "    elif \"prompt\" in record:\n",
    "        prompt = record.get(\"prompt\", \"\").strip()\n",
    "        if prompt and len(prompt.split()) > 8:\n",
    "            texts.append(prompt)\n",
    "    \n",
    "    # Fallback for other dataset types\n",
    "    elif dataset_name == \"microsoft/orca-math-word-problems-200k\":\n",
    "        question = record.get(\"question\", \"\").strip()\n",
    "        answer = record.get(\"answer\", \"\").strip()\n",
    "        if question and len(question.split()) > 8:\n",
    "            texts.append(question)\n",
    "        if answer and len(answer.split()) > 8:\n",
    "            texts.append(answer)\n",
    "    \n",
    "    elif dataset_name == \"tatsu-lab/alpaca_cleaned\":\n",
    "        instruction = record.get(\"instruction\", \"\").strip()\n",
    "        output = record.get(\"output\", \"\").strip()\n",
    "        if instruction and len(instruction.split()) > 5:\n",
    "            texts.append(instruction)\n",
    "        if output and len(output.split()) > 8:\n",
    "            texts.append(output)\n",
    "    \n",
    "    elif dataset_name == \"humarin/chatgpt-paraphrases\":\n",
    "        # Handle paraphrase format\n",
    "        original = record.get(\"text\", \"\").strip()\n",
    "        paraphrases_raw = record.get(\"paraphrases\", [])\n",
    "        \n",
    "        if original:\n",
    "            texts.append(original)\n",
    "        \n",
    "        # Parse paraphrases\n",
    "        if isinstance(paraphrases_raw, str) and paraphrases_raw.startswith('['):\n",
    "            try:\n",
    "                paraphrases = ast.literal_eval(paraphrases_raw)\n",
    "                for para in paraphrases[:2]:\n",
    "                    if para and len(str(para).split()) > 5:\n",
    "                        texts.append(str(para))\n",
    "            except:\n",
    "                pass\n",
    "        elif isinstance(paraphrases_raw, list):\n",
    "            for para in paraphrases_raw[:2]:\n",
    "                if para and len(str(para).split()) > 5:\n",
    "                    texts.append(str(para))\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def is_good_training_pair(formal_text, natural_text):\n",
    "    \"\"\"Validate training pair quality for Llama\"\"\"\n",
    "    \n",
    "    # Length checks\n",
    "    formal_words = len(formal_text.split())\n",
    "    natural_words = len(natural_text.split())\n",
    "    \n",
    "    if formal_words < 8 or natural_words < 8:\n",
    "        return False, \"too_short\"\n",
    "    if formal_words > 200 or natural_words > 200:\n",
    "        return False, \"too_long\"\n",
    "    \n",
    "    # Should be meaningfully different\n",
    "    if formal_text.lower() == natural_text.lower():\n",
    "        return False, \"identical\"\n",
    "    \n",
    "    # Check overlap (should have reasonable similarity)\n",
    "    formal_words_set = set(formal_text.lower().split())\n",
    "    natural_words_set = set(natural_text.lower().split())\n",
    "    overlap = len(formal_words_set & natural_words_set) / len(formal_words_set | natural_words_set)\n",
    "    \n",
    "    if overlap < 0.25:\n",
    "        return False, \"too_different\"\n",
    "    if overlap > 0.95:\n",
    "        return False, \"too_similar\"\n",
    "    \n",
    "    # Quality checks\n",
    "    if len(formal_text.strip()) < 20 or len(natural_text.strip()) < 20:\n",
    "        return False, \"too_short_chars\"\n",
    "    \n",
    "    return True, \"good\"\n",
    "\n",
    "# Process dataset for Llama training\n",
    "conversations = []\n",
    "skipped_counts = Counter()\n",
    "\n",
    "print(\"üîÑ Creating Llama-3.1 instruction dataset for humanization...\")\n",
    "print(f\"üìä Source: {dataset_info['name']}\")\n",
    "print(f\"üì¶ Total records: {len(ds):,}\")\n",
    "\n",
    "processed_count = 0\n",
    "for i, record in enumerate(ds):\n",
    "    if len(conversations) >= MAX_SAMPLES:\n",
    "        break\n",
    "    \n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        print(f\"  Processed {i:,} records, created {len(conversations):,} conversations...\")\n",
    "    \n",
    "    # Extract texts from record using the actual structure\n",
    "    texts = extract_text_from_record(record, dataset_info['name'])\n",
    "    \n",
    "    if not texts:\n",
    "        skipped_counts[\"no_text_extracted\"] += 1\n",
    "        continue\n",
    "    \n",
    "    for text in texts:\n",
    "        if len(conversations) >= MAX_SAMPLES:\n",
    "            break\n",
    "        \n",
    "        # Clean text\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        # Create formal version\n",
    "        formal_version = create_formal_version(text)\n",
    "        \n",
    "        # Validate pair\n",
    "        is_valid, reason = is_good_training_pair(formal_version, text)\n",
    "        if not is_valid:\n",
    "            skipped_counts[reason] += 1\n",
    "            continue\n",
    "        \n",
    "        # Format as Llama conversation\n",
    "        conversation = format_llama_instruction(formal_version, text)\n",
    "        conversations.append(conversation)\n",
    "        \n",
    "        processed_count += 1\n",
    "\n",
    "print(f\"\\n‚úÖ LLAMA DATASET CREATION COMPLETE!\")\n",
    "print(f\"üìä Total conversations: {len(conversations):,}\")\n",
    "print(f\"üéØ Target achieved: {len(conversations) >= MAX_SAMPLES}\")\n",
    "\n",
    "print(f\"\\nüìà PROCESSING STATS:\")\n",
    "total_processed = sum(skipped_counts.values()) + len(conversations)\n",
    "print(f\"  Records processed: {processed_count:,}\")\n",
    "print(f\"  Valid conversations: {len(conversations):,}\")\n",
    "print(f\"  Success rate: {len(conversations)/max(total_processed,1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  SKIPPED REASONS:\")\n",
    "for reason, count in skipped_counts.most_common():\n",
    "    print(f\"  {reason}: {count:,}\")\n",
    "\n",
    "if len(conversations) == 0:\n",
    "    print(f\"\\n‚ùå No valid conversations created!\")\n",
    "    print(f\"üí° Dataset structure analysis:\")\n",
    "    print(f\"   Available fields: {ds.column_names}\")\n",
    "    sample = ds[0]\n",
    "    print(f\"   Sample record: {sample}\")\n",
    "    raise ValueError(\"No valid conversations created! Check dataset structure.\")\n",
    "\n",
    "# Quality analysis\n",
    "avg_length = sum(len(conv) for conv in conversations) / len(conversations)\n",
    "print(f\"\\nüìä QUALITY METRICS:\")\n",
    "print(f\"  Average conversation length: {avg_length:.0f} characters\")\n",
    "print(f\"  Unique conversations: {len(set(conversations)):,}\")\n",
    "\n",
    "# Show sample conversation\n",
    "print(f\"\\nüìù SAMPLE LLAMA CONVERSATION:\")\n",
    "print(\"=\" * 80)\n",
    "sample_conv = conversations[0]\n",
    "if len(sample_conv) > 1200:\n",
    "    print(sample_conv[:600])\n",
    "    print(\"\\n... [middle content truncated] ...\\n\")\n",
    "    print(sample_conv[-600:])\n",
    "else:\n",
    "    print(sample_conv)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_conversations, val_conversations = train_test_split(\n",
    "    conversations, test_size=0.1, random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ DATASET SPLIT:\")\n",
    "print(f\"  Training conversations: {len(train_conversations):,}\")\n",
    "print(f\"  Validation conversations: {len(val_conversations):,}\")\n",
    "print(f\"\\n‚úÖ Llama-3.1 dataset ready for tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93de0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 ‚Äî Load Free Instruction Model and Tokenizer\n",
    "print(f\"üöÄ Loading {MODEL_NAME}...\")\n",
    "\n",
    "def try_load_model(model_name):\n",
    "    \"\"\"Try to load model with error handling and fallbacks\"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Attempting to load: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            padding_side=\"left\",  # Better for generation\n",
    "            truncation_side=\"right\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Add pad token if missing\n",
    "        if tokenizer.pad_token is None:\n",
    "            if tokenizer.eos_token is not None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            else:\n",
    "                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        \n",
    "        print(f\"‚úÖ Tokenizer loaded successfully!\")\n",
    "        print(f\"üìè Vocab size: {len(tokenizer):,}\")\n",
    "        print(f\"üè∑Ô∏è  Pad token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "        \n",
    "        # Load model with optimized settings\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "            \"device_map\": \"auto\" if DEVICE == \"cuda\" else None,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"low_cpu_mem_usage\": True\n",
    "        }\n",
    "        \n",
    "        # Add quantization for large models if needed\n",
    "        if \"7B\" in model_name or \"large\" in model_name:\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "                if gpu_memory < 16:  # Less than 16GB VRAM\n",
    "                    print(\"‚ö†Ô∏è  Large model + limited memory - using 8bit quantization\")\n",
    "                    try:\n",
    "                        from transformers import BitsAndBytesConfig\n",
    "                        quantization_config = BitsAndBytesConfig(\n",
    "                            load_in_8bit=True,\n",
    "                            llm_int8_threshold=6.0,\n",
    "                        )\n",
    "                        model_kwargs[\"quantization_config\"] = quantization_config\n",
    "                        print(\"‚úÖ 8-bit quantization enabled\")\n",
    "                    except ImportError:\n",
    "                        print(\"‚ö†Ô∏è  bitsandbytes not available. Install with: pip install bitsandbytes\")\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "        \n",
    "        # Move to device if not using device_map\n",
    "        if model_kwargs.get(\"device_map\") is None and DEVICE == \"cpu\":\n",
    "            base_model = base_model.to(DEVICE)\n",
    "        \n",
    "        return tokenizer, base_model, model_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {model_name}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Try to load the selected model, with fallbacks\n",
    "tokenizer = None\n",
    "base_model = None\n",
    "loaded_model_name = None\n",
    "\n",
    "# First, try the auto-selected model\n",
    "tokenizer, base_model, loaded_model_name = try_load_model(MODEL_NAME)\n",
    "\n",
    "# If that fails, try fallbacks\n",
    "if tokenizer is None:\n",
    "    print(f\"\\nüîÑ Trying fallback models...\")\n",
    "    \n",
    "    for fallback_model in FALLBACK_MODELS:\n",
    "        print(f\"\\nüì¶ Trying fallback: {fallback_model}\")\n",
    "        tokenizer, base_model, loaded_model_name = try_load_model(fallback_model)\n",
    "        \n",
    "        if tokenizer is not None:\n",
    "            print(f\"‚úÖ Successfully loaded fallback model: {fallback_model}\")\n",
    "            MODEL_NAME = fallback_model  # Update global variable\n",
    "            break\n",
    "    \n",
    "if tokenizer is None or base_model is None:\n",
    "    raise ValueError(\"‚ùå Could not load any model! Check your internet connection and try again.\")\n",
    "\n",
    "print(f\"\\nüéâ MODEL SUCCESSFULLY LOADED!\")\n",
    "print(f\"üì¶ Model: {loaded_model_name}\")\n",
    "print(f\"üîß Device: {base_model.device if hasattr(base_model, 'device') else 'Unknown'}\")\n",
    "print(f\"üìä Model dtype: {base_model.dtype if hasattr(base_model, 'dtype') else 'Unknown'}\")\n",
    "\n",
    "# Calculate parameters\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(f\"üìà Total parameters: {total_params:,} ({total_params/1e6:.0f}M)\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "    memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"üíæ GPU memory used: {memory_used:.1f} GB / {memory_total:.1f} GB\")\n",
    "    print(f\"üìä Memory usage: {memory_used/memory_total*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ MODEL READY FOR LORA CONFIGURATION!\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"Hello, this is a test.\"\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"\\nüß™ Tokenization test:\")\n",
    "print(f\"   Input: '{test_text}'\")\n",
    "print(f\"   Tokens: {test_tokens['input_ids'].shape}\")\n",
    "print(f\"   Decoded: '{tokenizer.decode(test_tokens['input_ids'][0])}'\")\n",
    "\n",
    "# Update global variables for consistency\n",
    "MODEL_NAME = loaded_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 ‚Äî Optimized LoRA Configuration for Free Models\n",
    "print(f\"üîß Configuring LoRA for {MODEL_NAME}...\")\n",
    "\n",
    "# Prepare model for LoRA training\n",
    "if hasattr(base_model, 'enable_input_require_grads'):\n",
    "    base_model.enable_input_require_grads()\n",
    "\n",
    "# Try quantization preparation if model was quantized\n",
    "try:\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "    print(\"‚úÖ Model prepared for quantized training\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è  Model preparation skipped (not quantized)\")\n",
    "\n",
    "base_model.train()\n",
    "\n",
    "# ADAPTIVE LORA CONFIG based on model size\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "model_size = total_params / 1e6  # Size in millions\n",
    "\n",
    "# Adjust LoRA config based on model size\n",
    "if model_size > 1000:  # Large models (1B+)\n",
    "    lora_r = 32\n",
    "    lora_alpha = 64\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "elif model_size > 500:  # Medium models (500M-1B)\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "else:  # Small models (<500M)\n",
    "    lora_r = 8\n",
    "    lora_alpha = 16\n",
    "    target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "# Detect model architecture and adjust target modules\n",
    "model_config = base_model.config\n",
    "architecture = model_config.architectures[0] if hasattr(model_config, 'architectures') else \"unknown\"\n",
    "\n",
    "print(f\"üèóÔ∏è  Detected architecture: {architecture}\")\n",
    "print(f\"üìè Model size: {model_size:.0f}M parameters\")\n",
    "\n",
    "# Architecture-specific target modules\n",
    "if \"GPT\" in architecture or \"Gpt\" in architecture:\n",
    "    # GPT-style models (DialoGPT, GPT-2, etc.)\n",
    "    target_modules = [\"c_attn\", \"c_proj\"]  # GPT attention layers\n",
    "    print(\"üéØ Using GPT-style attention modules\")\n",
    "elif \"Mistral\" in architecture:\n",
    "    # Mistral models\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    print(\"üéØ Using Mistral-style modules\")\n",
    "elif \"Llama\" in architecture or \"TinyLlama\" in architecture:\n",
    "    # Llama-style models\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    print(\"üéØ Using Llama-style modules\")\n",
    "else:\n",
    "    # Generic approach - find linear layers\n",
    "    print(\"üîç Detecting linear layers...\")\n",
    "    target_modules = []\n",
    "    for name, module in base_model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            layer_name = name.split('.')[-1]\n",
    "            if layer_name not in target_modules and len(layer_name) < 20:\n",
    "                target_modules.append(layer_name)\n",
    "    \n",
    "    target_modules = target_modules[:6]  # Limit to 6 most common\n",
    "    print(f\"üéØ Auto-detected modules: {target_modules}\")\n",
    "\n",
    "# Create LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False\n",
    ")\n",
    "\n",
    "print(f\"\\nüìã OPTIMIZED LORA CONFIGURATION:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Architecture: {architecture}\")\n",
    "print(f\"  Model size: {model_size:.0f}M parameters\")\n",
    "print(f\"  LoRA rank (r): {lora_config.r}\")\n",
    "print(f\"  LoRA alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  LoRA dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "\n",
    "# Apply LoRA to model\n",
    "print(f\"\\nüîÑ Applying LoRA adapters...\")\n",
    "try:\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    print(\"‚úÖ LoRA adapters applied successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error applying LoRA: {e}\")\n",
    "    print(\"üîß Trying with reduced target modules...\")\n",
    "    \n",
    "    # Fallback with minimal target modules\n",
    "    lora_config.target_modules = target_modules[:2]  # Use only first 2 modules\n",
    "    try:\n",
    "        model = get_peft_model(base_model, lora_config)\n",
    "        print(f\"‚úÖ LoRA applied with reduced modules: {lora_config.target_modules}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå LoRA failed completely: {e2}\")\n",
    "        raise\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "try:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"‚úÖ Gradient checkpointing enabled\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è  Gradient checkpointing not available\")\n",
    "\n",
    "# Calculate parameter statistics\n",
    "trainable_params = 0\n",
    "total_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(f\"\\nüìä PARAMETER ANALYSIS:\")\n",
    "print(f\"  Total parameters: {total_params:,} ({total_params/1e6:.0f}M)\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
    "print(f\"  Trainable percentage: {100 * trainable_params / total_params:.3f}%\")\n",
    "print(f\"  Memory efficiency: {total_params/trainable_params:.1f}x reduction\")\n",
    "\n",
    "# Validation checks\n",
    "assert trainable_params > 0, \"‚ùå ERROR: No trainable parameters found!\"\n",
    "assert model.training, \"‚ùå ERROR: Model is not in training mode!\"\n",
    "\n",
    "print(f\"\\nüéØ LORA SUMMARY:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Memory check\n",
    "if torch.cuda.is_available():\n",
    "    current_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    max_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"\\nüíæ MEMORY STATUS:\")\n",
    "    print(f\"  Current: {current_memory:.1f} GB / {total_memory:.1f} GB\")\n",
    "    print(f\"  Peak: {max_memory:.1f} GB\")\n",
    "    print(f\"  Usage: {current_memory/total_memory*100:.1f}%\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n‚úÖ LORA SETUP COMPLETE!\")\n",
    "print(f\"üöÄ {MODEL_NAME} ready for humanization training!\")\n",
    "print(f\"üéØ Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
    "\n",
    "# Quick generation test\n",
    "print(f\"\\nüß™ Generation test...\")\n",
    "test_input = tokenizer(\"Hello, how\", return_tensors=\"pt\").to(model.device)\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        test_output = model.generate(\n",
    "            **test_input, \n",
    "            max_length=test_input.input_ids.shape[1] + 5,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    test_result = tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
    "    print(f\"Test result: '{test_result}'\")\n",
    "    print(\"‚úÖ Model generation verified!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Generation test failed: {e}\")\n",
    "    print(\"‚ÑπÔ∏è  This may be normal - training should still work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 ‚Äî Llama-3.1 Instruction Dataset Tokenization\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class LlamaInstructionDataset(TorchDataset):\n",
    "    \"\"\"Custom dataset for Llama instruction fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, conversations, tokenizer, max_length):\n",
    "        self.conversations = conversations\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.conversations[idx]\n",
    "        \n",
    "        # Tokenize the full conversation\n",
    "        encoded = self.tokenizer(\n",
    "            conversation,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        # We'll mask the instruction part during loss calculation\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Find the assistant response start\n",
    "        # Look for the pattern: assistant<|end_header_id|>\\n\\n\n",
    "        assistant_start_pattern = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        \n",
    "        try:\n",
    "            # Find where assistant response starts\n",
    "            conversation_text = conversation\n",
    "            assistant_pos = conversation_text.find(assistant_start_pattern)\n",
    "            if assistant_pos != -1:\n",
    "                # Find the actual response start (after the header)\n",
    "                response_start = conversation_text.find(\"\\n\\n\", assistant_pos) + 2\n",
    "                \n",
    "                # Tokenize just the part before the response\n",
    "                prefix_text = conversation_text[:response_start]\n",
    "                prefix_tokens = self.tokenizer(prefix_text, add_special_tokens=False)[\"input_ids\"]\n",
    "                prefix_length = len(prefix_tokens)\n",
    "                \n",
    "                # Mask the instruction part (set to -100 so it's ignored in loss)\n",
    "                labels[:prefix_length] = -100\n",
    "        except:\n",
    "            # If parsing fails, mask first 70% (rough estimate)\n",
    "            mask_length = int(len(labels) * 0.7)\n",
    "            labels[:mask_length] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"üîÑ Creating Llama instruction datasets...\")\n",
    "\n",
    "train_dataset = LlamaInstructionDataset(\n",
    "    train_conversations,\n",
    "    tokenizer,\n",
    "    MAX_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = LlamaInstructionDataset(\n",
    "    val_conversations,\n",
    "    tokenizer,\n",
    "    MAX_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DATASETS CREATED:\")\n",
    "print(f\"  Training samples: {len(train_dataset):,}\")\n",
    "print(f\"  Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"  Max sequence length: {MAX_LENGTH}\")\n",
    "\n",
    "# Test tokenization\n",
    "print(f\"\\nüîç TOKENIZATION TEST:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"  Input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"  Labels shape: {sample['labels'].shape}\")\n",
    "\n",
    "# Check how much of the sequence is masked\n",
    "labels = sample['labels']\n",
    "masked_tokens = (labels == -100).sum().item()\n",
    "total_tokens = len(labels)\n",
    "response_tokens = total_tokens - masked_tokens\n",
    "\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "print(f\"  Masked tokens (instruction): {masked_tokens}\")\n",
    "print(f\"  Response tokens (for training): {response_tokens}\")\n",
    "print(f\"  Response percentage: {response_tokens/total_tokens*100:.1f}%\")\n",
    "\n",
    "# Show a sample of the tokenized conversation\n",
    "print(f\"\\nüìù SAMPLE TOKENIZED CONVERSATION:\")\n",
    "input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(\"=\" * 80)\n",
    "print(input_text[:500])\n",
    "if len(input_text) > 500:\n",
    "    print(\"... (truncated)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show where the training starts (non-masked labels)\n",
    "non_masked_indices = (sample['labels'] != -100).nonzero(as_tuple=True)[0]\n",
    "if len(non_masked_indices) > 0:\n",
    "    start_idx = non_masked_indices[0].item()\n",
    "    response_tokens = sample['input_ids'][start_idx:start_idx+50]  # First 50 response tokens\n",
    "    response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "    print(f\"\\nüéØ TRAINING TARGET (first 50 tokens of response):\")\n",
    "    print(f\"'{response_text}'\")\n",
    "\n",
    "print(f\"\\n‚úÖ Tokenization complete! Ready for training setup.\")\n",
    "\n",
    "# Memory usage check\n",
    "if torch.cuda.is_available():\n",
    "    memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"üíæ Current GPU memory: {memory_used:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d525be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 ‚Äî Llama-3.1 Training Configuration\n",
    "# Data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # No masked language modeling for causal LM\n",
    "    pad_to_multiple_of=8,  # Optimize for tensor cores\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# LLAMA-3.1 OPTIMIZED TRAINING ARGUMENTS\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3_humanization_lora\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Batch and gradient settings\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,  # More frequent evaluation\n",
    "    save_strategy=\"steps\", \n",
    "    save_steps=250,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Memory and performance optimization\n",
    "    fp16=True if DEVICE == \"cuda\" else False,\n",
    "    dataloader_pin_memory=True if DEVICE == \"cuda\" else False,\n",
    "    dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "    \n",
    "    # Logging and monitoring\n",
    "    logging_dir=\"./logs/llama_humanization\",\n",
    "    logging_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    \n",
    "    # Advanced settings for Llama\n",
    "    ddp_find_unused_parameters=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    prediction_loss_only=True,\n",
    "    \n",
    "    # Disable features that can cause issues with instruction tuning\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=None,\n",
    "    \n",
    "    # Length and padding\n",
    "    max_steps=-1,  # Use epochs instead\n",
    "    ignore_data_skip=True\n",
    ")\n",
    "\n",
    "print(\"üéØ LLAMA-3.1 TRAINING CONFIGURATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ Output directory: {training_args.output_dir}\")\n",
    "print(f\"üìö Batch size per device: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"üîÑ Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"‚ö° Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"üî• Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"üß† Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"üìä LR scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"üîß Optimizer: {training_args.optim}\")\n",
    "print(f\"üíæ FP16: {training_args.fp16}\")\n",
    "print(f\"‚úÖ Gradient checkpointing: {training_args.gradient_checkpointing}\")\n",
    "\n",
    "# Calculate training metrics\n",
    "total_samples = len(train_dataset)\n",
    "effective_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "steps_per_epoch = total_samples // effective_batch_size\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "print(f\"\\nüìä TRAINING SCHEDULE:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üì¶ Training samples: {total_samples:,}\")\n",
    "print(f\"üìè Steps per epoch: {steps_per_epoch:,}\")\n",
    "print(f\"üéØ Total training steps: {total_steps:,}\")\n",
    "print(f\"üî• Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"üìä Eval every: {training_args.eval_steps} steps\")\n",
    "print(f\"üíæ Save every: {training_args.save_steps} steps\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: {total_steps * 2:.0f}-{total_steps * 5:.0f} minutes\")\n",
    "\n",
    "print(f\"\\nüí° LLAMA-3.1 OPTIMIZATION FEATURES:\")\n",
    "print(\"‚úÖ Instruction-following architecture\")\n",
    "print(\"‚úÖ High-rank LoRA for better adaptation\")\n",
    "print(\"‚úÖ Proper attention masking\")\n",
    "print(\"‚úÖ Gradient checkpointing for memory\")\n",
    "print(\"‚úÖ Cosine learning rate schedule\")\n",
    "print(\"‚úÖ Frequent evaluation and saving\")\n",
    "\n",
    "# Memory estimation\n",
    "if torch.cuda.is_available():\n",
    "    current_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\nüíæ MEMORY STATUS:\")\n",
    "    print(f\"Current usage: {current_memory:.1f} GB\")\n",
    "    print(f\"Estimated training peak: {current_memory * 1.5:.1f} GB\")\n",
    "    \n",
    "    gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if current_memory * 1.5 > gpu_total * 0.9:\n",
    "        print(\"‚ö†Ô∏è  WARNING: May exceed GPU memory during training!\")\n",
    "        print(\"üí° Consider reducing batch size or using quantization\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training configuration complete!\")\n",
    "print(f\"üöÄ Ready to create Llama trainer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281410dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 ‚Äî Create Trainer for Causal Language Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainable_params_check = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"‚úÖ TRAINER CREATED SUCCESSFULLY!\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params_check:,}\")\n",
    "print(f\"üì¶ Training samples: {len(train_dataset):,}\")\n",
    "print(f\"üî¨ Validation samples: {len(val_dataset):,}\")\n",
    "\n",
    "print(f\"üìè Steps per epoch: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)}\")\n",
    "print(f\"üéØ Total training steps: {(len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)) * EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 ‚Äî Debug Quick-Train (Smoke Test)\n",
    "print(\"Starting debug smoke test...\")\n",
    "debug_train = LlamaInstructionDataset(\n",
    "    train_conversations[:min(180, len(train_conversations))],\n",
    "    tokenizer,\n",
    "    MAX_LENGTH\n",
    ")\n",
    "debug_val = LlamaInstructionDataset(\n",
    "    val_conversations[:min(20, len(val_conversations))],\n",
    "    tokenizer,\n",
    "    MAX_LENGTH\n",
    ")\n",
    "\n",
    "debug_args = TrainingArguments(\n",
    "    output_dir=\"./debug_lora_adapter\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=LR,\n",
    "    fp16=True if DEVICE == \"cuda\" else False,\n",
    "    logging_steps=10,\n",
    "    seed=SEED,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "debug_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=debug_args,\n",
    "    train_dataset=debug_train,\n",
    "    eval_dataset=debug_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "debug_result = debug_trainer.train()\n",
    "debug_trainer.save_model(\"./debug_lora_adapter\")\n",
    "\n",
    "print(f\"\\nDebug training completed!\")\n",
    "print(f\"Final loss: {debug_result.training_loss:.4f}\")\n",
    "if hasattr(debug_result, 'metrics'):\n",
    "    print(f\"Metrics: {debug_result.metrics}\")\n",
    "\n",
    "# Check training losses\n",
    "state_path = \"./debug_lora_adapter/trainer_state.json\"\n",
    "if os.path.exists(state_path):\n",
    "    with open(state_path, \"r\") as f:\n",
    "        state = json.load(f)\n",
    "    if \"log_history\" in state:\n",
    "        losses = [entry.get(\"loss\") for entry in state[\"log_history\"] if \"loss\" in entry]\n",
    "        print(f\"Last few training losses: {losses[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 ‚Äî Full Training\n",
    "print(\"Starting full training...\")\n",
    "updated all ceprint(f\"Training on {len(train_dataset)} examples\")\n",
    "print(f\"Validating on {len(val_dataset)} examples\")\n",
    "print(f\"This will take some time...\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "trainer.save_model(\"./instruction_lora_humanizer_adapter\")\n",
    "print(f\"Adapter saved to: ./instruction_lora_humanizer_adapter\")\n",
    "\n",
    "try:\n",
    "    model.save_pretrained(\"./instruction_lora_humanizer_full\")\n",
    "    print(f\"Full model saved to: ./instruction_lora_humanizer_full\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save full model: {e}\")\n",
    "\n",
    "print(\"\\nTraining complete! Adapter ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10.5 ‚Äî Diagnose Training Failure (Run this if loss is 0.0)\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING FAILURE DIAGNOSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. CHECK TRAINABLE PARAMETERS:\")\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Trainable params: {trainable:,}\")\n",
    "if trainable == 0:\n",
    "    print(\"   ‚ùå PROBLEM: No trainable parameters!\")\n",
    "else:\n",
    "    print(\"   ‚úì Trainable parameters exist\")\n",
    "\n",
    "print(\"\\n2. CHECK DATASET EXTRACTION:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "if len(train_conversations) > 0:\n",
    "    sample_conv = train_conversations[0]\n",
    "    # Handle different conversation formats\n",
    "    if isinstance(sample_conv, dict):\n",
    "        if 'input' in sample_conv and 'output' in sample_conv:\n",
    "            print(f\"   Sample conversation length: {len(sample_conv['input']) + len(sample_conv['output'])} chars\")\n",
    "            print(f\"\\n   Input preview: {sample_conv['input'][:150]}...\")\n",
    "            print(f\"   Output preview: {sample_conv['output'][:150]}...\")\n",
    "        else:\n",
    "            print(f\"   Sample conversation keys: {list(sample_conv.keys())}\")\n",
    "            print(f\"   Sample conversation: {str(sample_conv)[:200]}...\")\n",
    "    else:\n",
    "        print(f\"   Sample conversation type: {type(sample_conv)}\")\n",
    "        print(f\"   Sample conversation: {str(sample_conv)[:200]}...\")\n",
    "else:\n",
    "    print(\"   No conversations available\")\n",
    "print(\"\\n3. CHECK IF INPUT == OUTPUT (should be different):\")\n",
    "try:\n",
    "    if len(train_conversations) >= 100:\n",
    "        if isinstance(train_conversations[0], dict) and 'input' in train_conversations[0]:\n",
    "            matches = sum(1 for conv in train_conversations[:100] if conv.get('input', '').strip() == conv.get('output', '').strip())\n",
    "            print(f\"   Identical pairs in first 100: {matches}/100\")\n",
    "            if matches > 50:\n",
    "                print(\"   ‚ùå PROBLEM: Too many identical input/output pairs!\")\n",
    "            else:\n",
    "                print(\"   ‚úì Input and output are different\")\n",
    "        else:\n",
    "            print(\"   ‚ÑπÔ∏è  Cannot check - conversation format not recognized\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è  Less than 100 samples available\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Error checking conversations: {e}\")\n",
    "adapter_path = \"./instruction_lora_humanizer_adapter\"\n",
    "print(\"\\n4. CHECK ADAPTER FILES:\")\n",
    "import os\n",
    "adapter_path = \"./flan_t5_lora_ultrafeedback_adapter_retrained\"\n",
    "if os.path.exists(adapter_path):\n",
    "    files = os.listdir(adapter_path)\n",
    "    adapter_bin = [f for f in files if 'adapter' in f.lower() and '.bin' in f]\n",
    "    print(f\"   Adapter files found: {len(adapter_bin)}\")\n",
    "    if adapter_bin:\n",
    "        size = os.path.getsize(os.path.join(adapter_path, adapter_bin[0]))\n",
    "        print(f\"   Adapter size: {size:,} bytes\")\n",
    "        if size < 1000:\n",
    "            print(\"   ‚ùå PROBLEM: Adapter file too small!\")\n",
    "        else:\n",
    "            print(\"   ‚úì Adapter file size looks reasonable\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nDIAGNOSIS COMPLETE\")\n",
    "print(\"If trainable params = 0, the model never trained.\")\n",
    "print(\"You need to re-run from Cell 5 to re-enable LoRA.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 ‚Äî Inference Load Adapter\n",
    "print(\"Loading model for inference...\")\n",
    "\n",
    "dtype = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "base_model_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    ")\n",
    "\n",
    "if DEVICE == \"cpu\":\n",
    "    base_model_inference = base_model_inference.to(DEVICE)\n",
    "\n",
    "model_inference = PeftModel.from_pretrained(\n",
    "    base_model_inference,\n",
    "    \"./instruction_lora_humanizer_adapter\"\n",
    ")\n",
    "\n",
    "model_inference.eval()\n",
    "\n",
    "print(f\"Model loaded and ready for inference on {DEVICE}\")\n",
    "print(f\"Model is in eval mode: {not model_inference.training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 ‚Äî Llama-3.1 Humanization Inference Functions\n",
    "def humanize_with_llama(\n",
    "    formal_text,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Humanize formal text using fine-tuned Llama-3.1\n",
    "    \n",
    "    Args:\n",
    "        formal_text: Input formal text to humanize\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        top_p: Nucleus sampling parameter\n",
    "        do_sample: Whether to use sampling\n",
    "        repetition_penalty: Penalty for repetitive text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create instruction prompt in Llama format\n",
    "    instruction_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an expert at rewriting formal, robotic text to sound natural and human. Make the text conversational, warm, and engaging while preserving the original meaning.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Rewrite this formal text to sound more natural and human:\n",
    "\n",
    "{formal_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        instruction_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH - max_new_tokens  # Leave room for response\n",
    "    ).to(model_inference.device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model_inference.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part (exclude input prompt)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    \n",
    "    # Decode and clean\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up response\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Remove any remaining special tokens or artifacts\n",
    "    response = response.replace(\"<|eot_id|>\", \"\").strip()\n",
    "    response = response.replace(\"<|end_of_text|>\", \"\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def batch_humanize_llama(texts, batch_size=2, **kwargs):\n",
    "    \"\"\"Efficiently humanize multiple texts with Llama\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"üîÑ Processing {len(texts)} texts in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_results = []\n",
    "        \n",
    "        for text in batch_texts:\n",
    "            try:\n",
    "                result = humanize_with_llama(text, **kwargs)\n",
    "                batch_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error processing text: {e}\")\n",
    "                batch_results.append(f\"Error: {text}\")\n",
    "        \n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        if len(results) % 10 == 0:\n",
    "            print(f\"  Completed {len(results)}/{len(texts)}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_humanization_quality(original, humanized):\n",
    "    \"\"\"Analyze the quality of humanization\"\"\"\n",
    "    \n",
    "    # Formality indicators\n",
    "    formal_indicators = [\n",
    "        'utilize', 'obtain', 'demonstrate', 'facilitate', 'implement',\n",
    "        'approximately', 'subsequently', 'furthermore', 'nevertheless',\n",
    "        'consequently', 'therefore', 'however', 'moreover', 'additionally',\n",
    "        'commence', 'terminate', 'investigate', 'establish', 'constitute'\n",
    "    ]\n",
    "    \n",
    "    casual_indicators = [\n",
    "        'use', 'get', 'show', 'help', 'do', 'about', 'then', 'also',\n",
    "        'but', 'so', 'though', 'and', 'plus', 'too', 'start', 'stop',\n",
    "        'look into', 'set up', 'make up'\n",
    "    ]\n",
    "    \n",
    "    # Count indicators\n",
    "    orig_words = original.lower().split()\n",
    "    hum_words = humanized.lower().split()\n",
    "    \n",
    "    orig_formal = sum(1 for word in orig_words if word in formal_indicators)\n",
    "    orig_casual = sum(1 for word in orig_words if word in casual_indicators)\n",
    "    \n",
    "    hum_formal = sum(1 for word in hum_words if word in formal_indicators)\n",
    "    hum_casual = sum(1 for word in hum_words if word in casual_indicators)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    formality_reduction = orig_formal - hum_formal\n",
    "    casualness_increase = hum_casual - orig_casual\n",
    "    length_change = len(hum_words) - len(orig_words)\n",
    "    \n",
    "    # Readability estimation (simple)\n",
    "    avg_word_length_orig = sum(len(word) for word in orig_words) / len(orig_words)\n",
    "    avg_word_length_hum = sum(len(word) for word in hum_words) / len(hum_words)\n",
    "    \n",
    "    return {\n",
    "        'original_formal_score': orig_formal,\n",
    "        'humanized_formal_score': hum_formal,\n",
    "        'original_casual_score': orig_casual,\n",
    "        'humanized_casual_score': hum_casual,\n",
    "        'formality_reduction': formality_reduction,\n",
    "        'casualness_increase': casualness_increase,\n",
    "        'length_change': length_change,\n",
    "        'avg_word_length_change': avg_word_length_orig - avg_word_length_hum,\n",
    "        'humanization_score': (formality_reduction + casualness_increase) / max(len(orig_words), 1) * 100\n",
    "    }\n",
    "\n",
    "def compare_responses(formal_text, **generation_kwargs):\n",
    "    \"\"\"Compare different generation strategies\"\"\"\n",
    "    \n",
    "    print(f\"üîç COMPARING GENERATION STRATEGIES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìù Original: {formal_text}\")\n",
    "    print()\n",
    "    \n",
    "    strategies = [\n",
    "        {\"name\": \"Conservative\", \"temperature\": 0.3, \"do_sample\": True},\n",
    "        {\"name\": \"Balanced\", \"temperature\": 0.7, \"do_sample\": True},\n",
    "        {\"name\": \"Creative\", \"temperature\": 1.0, \"do_sample\": True},\n",
    "        {\"name\": \"Deterministic\", \"do_sample\": False, \"temperature\": 1.0}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"üéØ {strategy['name']} ({strategy}):\")\n",
    "        try:\n",
    "            result = humanize_with_llama(formal_text, **{**generation_kwargs, **strategy})\n",
    "            results[strategy['name']] = result\n",
    "            print(f\"   {result}\")\n",
    "            \n",
    "            # Quick analysis\n",
    "            analysis = analyze_humanization_quality(formal_text, result)\n",
    "            print(f\"   üìä Humanization score: {analysis['humanization_score']:.1f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "            results[strategy['name']] = f\"Error: {e}\"\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"ü¶ô LLAMA-3.1 HUMANIZATION FUNCTIONS READY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìã AVAILABLE FUNCTIONS:\")\n",
    "print(\"  üéØ humanize_with_llama(text) - Main humanization function\")\n",
    "print(\"  üé≤ humanize_with_llama(text, temperature=1.0) - More creative\")\n",
    "print(\"  ‚ö° batch_humanize_llama(texts) - Process multiple texts\")\n",
    "print(\"  üìä analyze_humanization_quality(orig, hum) - Quality analysis\")\n",
    "print(\"  üîç compare_responses(text) - Test different strategies\")\n",
    "\n",
    "print(f\"\\nüí° USAGE EXAMPLES:\")\n",
    "print('  result = humanize_with_llama(\"This implementation demonstrates optimal functionality.\")')\n",
    "print('  results = batch_humanize_llama([\"Text 1\", \"Text 2\", \"Text 3\"])')\n",
    "print('  analysis = analyze_humanization_quality(original, humanized)')\n",
    "print('  comparison = compare_responses(\"Formal text here\")')\n",
    "\n",
    "print(f\"\\nüéõÔ∏è  GENERATION PARAMETERS:\")\n",
    "print(\"  ‚Ä¢ temperature: 0.1-1.5 (creativity level)\")\n",
    "print(\"  ‚Ä¢ top_p: 0.8-0.95 (nucleus sampling)\")\n",
    "print(\"  ‚Ä¢ repetition_penalty: 1.0-1.2 (avoid repetition)\")\n",
    "print(\"  ‚Ä¢ max_new_tokens: 50-500 (response length)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Llama-3.1 inference functions ready!\")\n",
    "print(\"üöÄ Perfect for natural, human-like text generation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606124de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 ‚Äî Interactive Testing\n",
    "test_text = input(\"Enter text to humanize (or press Enter for sample): \").strip()\n",
    "\n",
    "if not test_text:\n",
    "    test_text = \"The utilization of artificial intelligence in modern computational systems represents a paradigm shift in technological advancement.\"\n",
    "    print(f\"Using sample text: {test_text}\\n\")\n",
    "\n",
    "print(\"\\n=== CONSERVATIVE (temperature=0.3) ===\")\n",
    "conservative_result = humanize_with_llama(test_text, temperature=0.3)\n",
    "print(conservative_result)\n",
    "\n",
    "print(\"\\n=== BALANCED (temperature=0.7) ===\")\n",
    "balanced_result = humanize_with_llama(test_text, temperature=0.7)\n",
    "print(balanced_result)\n",
    "\n",
    "print(\"\\n=== CREATIVE (temperature=1.0) ===\")\n",
    "creative_result = humanize_with_llama(test_text, temperature=1.0)\n",
    "print(creative_result)\n",
    "\n",
    "print(\"\\n=== ORIGINAL ===\")\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71bcb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 ‚Äî Compare Base vs Fine-tuned\n",
    "comparison_text = \"The implementation of machine learning algorithms necessitates comprehensive data preprocessing and feature engineering methodologies.\"\n",
    "\n",
    "print(\"\\n=== ORIGINAL TEXT ===\")\n",
    "print(comparison_text)\n",
    "\n",
    "print(\"\\n=== FINE-TUNED LLAMA OUTPUT ===\")\n",
    "finetuned_result = humanize_with_llama(comparison_text, temperature=0.7)\n",
    "print(finetuned_result)\n",
    "\n",
    "print(\"\\n=== ANALYSIS ===\")\n",
    "analysis = analyze_humanization_quality(comparison_text, finetuned_result)\n",
    "print(f\"Humanization score: {analysis['humanization_score']:.1f}\")\n",
    "print(f\"Formality reduction: {analysis['formality_reduction']}\")\n",
    "print(f\"Casualness increase: {analysis['casualness_increase']}\")\n",
    "print(f\"Length change: {analysis['length_change']} words\")\n",
    "\n",
    "print(\"\\nComparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15 ‚Äî Verify Trainable Parameters\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Parameter verification:\")\n",
    "print(f\"  Total: {total:,}\")\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "print(f\"  Percentage: {100 * trainable / total:.3f}%\")\n",
    "\n",
    "print(\"\\nLoRA modules in model:\")\n",
    "lora_count = 0\n",
    "for name, module in model.named_modules():\n",
    "    if \"lora\" in name.lower():\n",
    "        print(f\"  {name}\")\n",
    "        lora_count += 1\n",
    "print(f\"\\nTotal LoRA modules: {lora_count}\")\n",
    "\n",
    "if trainable == 0:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: No trainable parameters! Re-enabling LoRA...\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(kw in name.lower() for kw in [\"lora\", \"lora_a\", \"lora_b\"]):\n",
    "            param.requires_grad = True\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Re-enabled {trainable:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bad2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16 ‚Äî Training Loss History\n",
    "state_path = \"./instruction_lora_humanizer_adapter/trainer_state.json\"\n",
    "\n",
    "if os.path.exists(state_path):\n",
    "    with open(state_path, \"r\") as f:\n",
    "        state = json.load(f)\n",
    "    \n",
    "    log_history = state.get(\"log_history\", [])\n",
    "    losses = [entry.get(\"loss\") for entry in log_history if \"loss\" in entry]\n",
    "    \n",
    "    print(f\"Training history loaded: {len(log_history)} entries\")\n",
    "    print(f\"Loss entries: {len(losses)}\")\n",
    "    \n",
    "    if losses:\n",
    "        print(f\"\\nLast 10 training losses:\")\n",
    "        for i, loss in enumerate(losses[-10:], 1):\n",
    "            print(f\"  {i}. {loss:.4f}\")\n",
    "        \n",
    "        print(f\"\\nFirst loss: {losses[0]:.4f}\")\n",
    "        print(f\"Last loss: {losses[-1]:.4f}\")\n",
    "        print(f\"Improvement: {losses[0] - losses[-1]:.4f}\")\n",
    "else:\n",
    "    print(f\"Trainer state file not found at {state_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc787cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17 ‚Äî Verify Saved Files\n",
    "output_dirs = [\n",
    "    \"./instruction_lora_humanizer_adapter\",\n",
    "    \"./debug_lora_adapter\",\n",
    "    \"./instruction_lora_humanizer_full\"\n",
    "]\n",
    "\n",
    "for dir_path in output_dirs:\n",
    "    if os.path.exists(dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        print(f\"\\n{dir_path}:\")\n",
    "        print(f\"  Files: {len(files)}\")\n",
    "        for f in sorted(files)[:10]:\n",
    "            size = os.path.getsize(os.path.join(dir_path, f))\n",
    "            print(f\"    {f} ({size:,} bytes)\")\n",
    "        if len(files) > 10:\n",
    "            print(f\"    ... and {len(files) - 10} more files\")\n",
    "    else:\n",
    "        print(f\"\\n{dir_path}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18 ‚Äî Visualize Training Loss\n",
    "state_path = \"./flan_t5_lora_ultrafeedback_retrain/trainer_state.json\"\n",
    "\n",
    "if os.path.exists(state_path):\n",
    "    with open(state_path, \"r\") as f:\n",
    "        state = json.load(f)\n",
    "    \n",
    "    log_history = state.get(\"log_history\", [])\n",
    "    \n",
    "    steps = []\n",
    "    losses = []\n",
    "    eval_losses = []\n",
    "    eval_steps = []\n",
    "    \n",
    "    for entry in log_history:\n",
    "        if \"loss\" in entry and \"step\" in entry:\n",
    "            steps.append(entry[\"step\"])\n",
    "            losses.append(entry[\"loss\"])\n",
    "        if \"eval_loss\" in entry and \"step\" in entry:\n",
    "            eval_steps.append(entry[\"step\"])\n",
    "            eval_losses.append(entry[\"eval_loss\"])\n",
    "    \n",
    "    if losses:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(steps, losses, label=\"Training Loss\", linewidth=2, alpha=0.8)\n",
    "        if eval_losses:\n",
    "            plt.plot(eval_steps, eval_losses, label=\"Validation Loss\", linewidth=2, alpha=0.8, marker='o')\n",
    "        plt.xlabel(\"Steps\", fontsize=12)\n",
    "        plt.ylabel(\"Loss\", fontsize=12)\n",
    "        plt.title(\"Training Progress\", fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nPlotted {len(losses)} training loss points\")\n",
    "        if eval_losses:\n",
    "            print(f\"Plotted {len(eval_losses)} validation loss points\")\n",
    "    else:\n",
    "        print(\"No loss data found to plot\")\n",
    "else:\n",
    "    print(f\"Cannot plot: {state_path} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa387323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19 ‚Äî Fix Trainable Parameters (Emergency Recovery)\n",
    "print(\"Emergency LoRA parameter recovery utility\")\n",
    "print(\"Run this cell if trainable_params becomes 0\\n\")\n",
    "\n",
    "before_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters before fix: {before_count:,}\")\n",
    "\n",
    "lora_keywords = [\"lora\", \"lora_a\", \"lora_b\", \"lora_embedding\", \"lora_magnitude\"]\n",
    "fixed_count = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(keyword in name.lower() for keyword in lora_keywords):\n",
    "        if not param.requires_grad:\n",
    "            param.requires_grad = True\n",
    "            fixed_count += 1\n",
    "            print(f\"  Re-enabled: {name}\")\n",
    "\n",
    "after_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTrainable parameters after fix: {after_count:,}\")\n",
    "print(f\"Parameters re-enabled: {fixed_count}\")\n",
    "\n",
    "if after_count > 0:\n",
    "    print(\"\\n‚úì LoRA parameters successfully recovered!\")\n",
    "    print(\"  You can now recreate the trainer and continue training.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Still no trainable parameters. Check model configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20 ‚Äî OOM Recovery Instructions\n",
    "print(\"=\" * 60)\n",
    "print(\"OUT OF MEMORY (OOM) TROUBLESHOOTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nIf you encounter CUDA OOM errors, try these fixes:\\n\")\n",
    "print(\"1. REDUCE BATCH SIZE\")\n",
    "print(\"   Change: BATCH_SIZE = 2  (in Cell 1)\")\n",
    "print(\"   Then re-run cells 7 and 8 to recreate trainer\\n\")\n",
    "print(\"2. REDUCE MAX LENGTHS\")\n",
    "print(\"   Change: MAX_SOURCE_LENGTH = 128\")\n",
    "print(\"   Change: MAX_TARGET_LENGTH = 128\")\n",
    "print(\"   Then re-run tokenization (Cell 6) and trainer (Cells 7-8)\\n\")\n",
    "print(\"3. REDUCE SAMPLE COUNT\")\n",
    "print(\"   Change: MAX_SAMPLES = 10000\")\n",
    "print(\"   Then re-run from Cell 3 onwards\\n\")\n",
    "print(\"4. USE GRADIENT ACCUMULATION\")\n",
    "print(\"   In Cell 7, add to training_args:\")\n",
    "print(\"   gradient_accumulation_steps=4\")\n",
    "print(\"   Then recreate trainer (Cell 8)\\n\")\n",
    "print(\"5. CLEAR CACHE\")\n",
    "print(\"   Run: torch.cuda.empty_cache()\")\n",
    "print(\"   Run: import gc; gc.collect()\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nCurrent GPU memory:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    print(f\"  Max allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26214f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21 ‚Äî Optional: Hugging Face Login (Commented)\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# print(\"Logged in to Hugging Face!\")\n",
    "# print(\"This is optional - only needed for private models/datasets\")\n",
    "# print(\"or to push your fine-tuned model to the Hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07204f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22 ‚Äî Llama-3.1 Humanization Complete Setup Summary\n",
    "print(\"ü¶ô\" * 80)\n",
    "print(\"LLAMA-3.1-8B-INSTRUCT HUMANIZATION - SETUP COMPLETE!\")\n",
    "print(\"ü¶ô\" * 80)\n",
    "\n",
    "print(f\"\\nüéØ WHY LLAMA-3.1 IS PERFECT FOR HUMANIZATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Native instruction understanding - Follows humanization commands naturally\")\n",
    "print(\"‚úÖ Superior style adaptation - Trained on diverse human writing styles\")  \n",
    "print(\"‚úÖ Conversational architecture - Designed for natural dialogue\")\n",
    "print(\"‚úÖ Excellent LoRA adaptation - Fast convergence with fewer examples\")\n",
    "print(\"‚úÖ Advanced context handling - Understands nuanced rewriting requests\")\n",
    "print(\"‚úÖ Human preference training - Aligned with human writing preferences\")\n",
    "\n",
    "print(f\"\\nü§ñ MODEL CONFIGURATION:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  üì¶ Base: {MODEL_NAME}\")\n",
    "print(f\"  üéØ Task: Formal ‚Üí Natural human text\")\n",
    "print(f\"  üîß Method: High-rank LoRA (r={LORA_R}, Œ±={LORA_ALPHA})\")\n",
    "print(f\"  üìè Context: {MAX_LENGTH:,} tokens\")\n",
    "print(f\"  üìä Training samples: {len(train_conversations) if 'train_conversations' in dir() else 'N/A'}\")\n",
    "print(f\"  üî¨ Validation samples: {len(val_conversations) if 'val_conversations' in dir() else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nüèÉ TRAINING STRATEGY:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"  üî• Epochs: {EPOCHS} (Llama learns fast)\")\n",
    "print(f\"  üìö Batch: {BATCH_SIZE} √ó {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION} effective\")\n",
    "print(f\"  üß† Learning rate: {LR} (optimized for instruct)\")\n",
    "print(f\"  ‚ö° Scheduler: Cosine with warmup\")\n",
    "print(f\"  üíæ Memory: Gradient checkpointing + FP16\")\n",
    "print(f\"  üéØ Focus: Instruction following & style transfer\")\n",
    "\n",
    "print(f\"\\nüìä DATASET ENGINEERING:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"  üîÑ Smart formal‚Üínatural conversion\")\n",
    "print(\"  üìù Proper Llama instruction format\")\n",
    "print(\"  üé≠ Multiple conversation styles\")\n",
    "print(\"  ‚úÇÔ∏è  Intelligent response masking\")\n",
    "print(\"  üé® Quality filtering & validation\")\n",
    "print(\"  üìè Optimal length distribution\")\n",
    "\n",
    "print(f\"\\nüíæ TRAINING OUTPUTS:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"  üìÅ ./llama3_humanization_lora/ (LoRA adapters)\")\n",
    "print(\"  üìä ./logs/llama_humanization/ (TensorBoard)\")\n",
    "print(\"  üîç Model checkpoints every 250 steps\")\n",
    "print(\"  üìà Evaluation metrics & loss tracking\")\n",
    "\n",
    "print(f\"\\nüöÄ INFERENCE CAPABILITIES:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"  üéØ humanize_with_llama() - Natural rewriting\")\n",
    "print(\"  üé≤ Temperature control - Creativity levels\")\n",
    "print(\"  ‚ö° Batch processing - Efficient bulk ops\")\n",
    "print(\"  üìä Quality analysis - Formality metrics\")\n",
    "print(\"  üîç Strategy comparison - A/B testing\")\n",
    "\n",
    "print(f\"\\nüìà EXPECTED PERFORMANCE:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"  ‚ö° Fast convergence: ~500-1000 steps\")\n",
    "print(\"  üéØ High quality: Natural, engaging output\")\n",
    "print(\"  üîß Flexible control: Multiple generation modes\")\n",
    "print(\"  üí™ Robust: Handles various text types\")\n",
    "print(\"  üé® Creative: Maintains personality & style\")\n",
    "\n",
    "print(f\"\\nüí° ADVANTAGES OVER T5/FLAN-T5:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"  üß† Better instruction comprehension\")\n",
    "print(\"  üé≠ Superior style & tone adaptation\")  \n",
    "print(\"  üí¨ More natural conversational flow\")\n",
    "print(\"  üöÄ Faster training convergence\")\n",
    "print(\"  üéØ Higher humanization quality\")\n",
    "print(\"  üîß More controllable generation\")\n",
    "\n",
    "print(f\"\\nüìã NEXT STEPS:\")\n",
    "print(\"=\" * 20)\n",
    "print(\"  1. üèÉ Run training (Cells 8-10)\")\n",
    "print(\"  2. üìä Monitor TensorBoard logs\")\n",
    "print(\"  3. üß™ Test humanization (Cell 13)\")\n",
    "print(\"  4. üîç Compare quality (Cell 14)\")\n",
    "print(\"  5. ‚öôÔ∏è  Fine-tune generation params\")\n",
    "print(\"  6. üöÄ Deploy for production use\")\n",
    "print(\"  7. üìà Scale to larger datasets\")\n",
    "\n",
    "print(f\"\\nüéä LLAMA-3.1 HUMANIZATION PIPELINE READY!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ChatGPT was absolutely right - Llama-3.1 is THE model for humanization!\")\n",
    "print(\"ü¶ô Start training and experience the difference! ü¶ô\")\n",
    "print(\"ü¶ô\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
